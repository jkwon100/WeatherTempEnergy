{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15cdfa2b-ee53-4d98-921c-5191630221d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Region  Entry_Count Start_Date   End_Date\n",
      "0     COMED          945 1998-01-01 2013-09-01\n",
      "1      DEOK          189 1998-01-01 2013-09-01\n",
      "2       DOM          756 1998-01-01 2013-09-01\n",
      "3       DUQ          189 1998-01-01 2013-09-01\n",
      "4        FE          189 1998-01-01 2013-09-01\n",
      "5        NI          378 1998-01-01 2013-09-01\n",
      "6      PJME          756 1998-01-01 2013-09-01\n",
      "7      PJMW          945 1998-01-01 2013-09-01\n",
      "8  PJM_Load         1701 1998-01-01 2013-09-01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "temperature_path = \"temperatureData_clean/US_Temp_City_RegionOnly_1998_2019.csv\"\n",
    "df = pd.read_csv(temperature_path)\n",
    "\n",
    "# Ensure 'Date' is in datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Group by Region and get count, min, and max of Date\n",
    "region_summary = df.groupby(\"Region\").agg(\n",
    "    Entry_Count=(\"Date\", \"count\"),\n",
    "    Start_Date=(\"Date\", \"min\"),\n",
    "    End_Date=(\"Date\", \"max\")\n",
    ").reset_index()\n",
    "\n",
    "# Display result\n",
    "print(region_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9594940-f1c1-4111-9e64-5b6320475540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File                                   Entries      Start Date        End Date\n",
      "--------------------------------------------------------------------------------\n",
      "AEP_hourly_daily.csv                      5055      2004-10-01      2018-08-03\n",
      "COMED_hourly_daily.csv                    2772      2011-01-01      2018-08-03\n",
      "DAYTON_hourly_daily.csv                   5055      2004-10-01      2018-08-03\n",
      "DEOK_hourly_daily.csv                     2407      2012-01-01      2018-08-03\n",
      "DOM_hourly_daily.csv                      4843      2005-05-01      2018-08-03\n",
      "DUQ_hourly_daily.csv                      4963      2005-01-01      2018-08-03\n",
      "EKPC_hourly_daily.csv                     1890      2013-06-01      2018-08-03\n",
      "FE_hourly_daily.csv                       2621      2011-06-01      2018-08-03\n",
      "NI_hourly_daily.csv                       2437      2004-05-01      2011-01-01\n",
      "PJM_Load_cleaned_daily.csv                1372      1998-04-01      2002-01-01\n",
      "PJM_Load_hourly_daily.csv                 1372      1998-04-01      2002-01-01\n",
      "PJME_hourly_daily.csv                     6059      2002-01-01      2018-08-03\n",
      "PJMW_hourly_daily.csv                     5969      2002-04-01      2018-08-03\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder path\n",
    "folder_path = \"energyData_clean\"\n",
    "\n",
    "# File names\n",
    "file_names = [\n",
    "    \"AEP_hourly_daily.csv\",\n",
    "    \"COMED_hourly_daily.csv\",\n",
    "    \"DAYTON_hourly_daily.csv\",\n",
    "    \"DEOK_hourly_daily.csv\",\n",
    "    \"DOM_hourly_daily.csv\",\n",
    "    \"DUQ_hourly_daily.csv\",\n",
    "    \"EKPC_hourly_daily.csv\",\n",
    "    \"FE_hourly_daily.csv\",\n",
    "    \"NI_hourly_daily.csv\",\n",
    "    \"PJM_Load_cleaned_daily.csv\",\n",
    "    \"PJM_Load_hourly_daily.csv\",\n",
    "    \"PJME_hourly_daily.csv\",\n",
    "    \"PJMW_hourly_daily.csv\"\n",
    "]\n",
    "\n",
    "# Header\n",
    "print(f\"{'File':<35} {'Entries':>10} {'Start Date':>15} {'End Date':>15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Loop through files\n",
    "for filename in file_names:\n",
    "    path = os.path.join(folder_path, filename)\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        date_col = next((col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()), None)\n",
    "\n",
    "        if date_col:\n",
    "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "            df = df.dropna(subset=[date_col])\n",
    "            entry_count = len(df)\n",
    "            start_date = df[date_col].min().strftime('%Y-%m-%d')\n",
    "            end_date = df[date_col].max().strftime('%Y-%m-%d')\n",
    "            print(f\"{filename:<35} {entry_count:>10} {start_date:>15} {end_date:>15}\")\n",
    "        else:\n",
    "            print(f\"{filename:<35} {'--':>10} {'No date column':>15} {'':>15}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{filename:<35} {'--':>10} {'Error':>15} {str(e)[:25]:>15}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926cade-f91a-443c-b0c9-ad9e3efb8fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
