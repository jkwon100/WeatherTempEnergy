{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68d02cc",
   "metadata": {},
   "source": [
    "# Energy Prediction Notebook\n",
    "\n",
    "This notebook demonstrates a comprehensive workflow for forecasting regional energy demand using a variety of machine learning models. The workflow includes robust feature engineering, model training, evaluation, and comparison across multiple advanced regression algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## **Workflow Overview**\n",
    "\n",
    "1. **Data Preparation**\n",
    "    - Regional energy consumption and weather data are loaded and merged.\n",
    "    - Data is split into training and test sets for each region.\n",
    "\n",
    "2. **Feature Engineering**\n",
    "    - **Lagged Features:** Past values of temperature and energy are added as features to capture temporal dependencies.\n",
    "    - **Calendar Features:** Month, day of week, and weekend indicators are extracted to model seasonality and weekly patterns.\n",
    "    - **Sin/Cos Seasonality:** Sine and cosine transformations of the day of year are used to encode annual seasonality in a continuous, cyclical manner.\n",
    "    - **Interaction Features:** Multiplicative interactions between temperature and lagged energy are included to capture non-linear effects.\n",
    "    - **Dimensionality Reduction & Selection (in pipelines):** Feature selection (SelectKBest) and Principal Component Analysis (PCA) are used in some pipelines to reduce noise and multicollinearity.\n",
    "\n",
    "3. **Model Training & Evaluation**\n",
    "    - Multiple machine learning models are trained for each region.\n",
    "    - Performance is evaluated using RMSE (Root Mean Squared Error) and R² (coefficient of determination).\n",
    "    - Results are visualized and compared across models and regions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Models Implemented**\n",
    "\n",
    "1. **XGBoost Regressor**\n",
    "    - A powerful gradient boosting tree model, robust to outliers and capable of capturing complex relationships.\n",
    "\n",
    "2. **Support Vector Regression (SVR)**\n",
    "    - A kernel-based regression model effective for non-linear relationships.\n",
    "\n",
    "3. **Random Forest Regressor**\n",
    "    - An ensemble of decision trees that reduces overfitting and improves generalization.\n",
    "\n",
    "4. **Linear Regression**\n",
    "    - A simple, interpretable baseline model for linear relationships.\n",
    "\n",
    "5. **LightGBM Regressor**\n",
    "    - A fast, efficient gradient boosting framework optimized for large datasets.\n",
    "\n",
    "6. **Gradient Boosting Regressor**\n",
    "    - Another tree-based boosting model, similar to XGBoost but from scikit-learn.\n",
    "\n",
    "7. **Ridge Regression**\n",
    "    - Linear regression with L2 regularization to handle multicollinearity and prevent overfitting.\n",
    "\n",
    "8. **SVR Pipeline**\n",
    "    - A pipeline combining scaling, feature selection, PCA, and SVR to improve performance by preprocessing features.\n",
    "\n",
    "9. **XGBoost + SVR Stacking Ensemble**\n",
    "    - A stacking regressor that combines predictions from an SVR pipeline and XGBoost, using another XGBoost as a meta-learner for improved accuracy.\n",
    "\n",
    "10. **LSTM Neural Network**\n",
    "     - A deep learning model (Long Short-Term Memory) designed for sequential data, capturing long-term temporal dependencies in energy demand.\n",
    "\n",
    "---\n",
    "\n",
    "## **Feature Engineering**\n",
    "\n",
    "- **Lagged Features:**  \n",
    "  - `Temp_lag1`, `Temp_lag2`, `Temp_lag3`: Previous days' temperatures.\n",
    "  - `Energy_lag1`, `Energy_lag2`, `Energy_lag3`, `Energy_lag7`: Previous days' energy consumption.\n",
    "\n",
    "- **Calendar Features:**  \n",
    "  - `month`, `dayofweek`, `is_weekend`: Encodes seasonality and weekly cycles.\n",
    "\n",
    "- **Seasonality (Sin/Cos):**  \n",
    "  - `sin_DOY`, `cos_DOY`: Encodes cyclical annual patterns.\n",
    "\n",
    "- **Interaction Features:**  \n",
    "  - `Temp_x_lag1`, `Temp_x_lag2`: Captures non-linear effects between temperature and past energy.\n",
    "\n",
    "- **Advanced Pipelines:**  \n",
    "  - **Scaling:** Standardizes features for models sensitive to feature scale (e.g., SVR).\n",
    "  - **Feature Selection:** Selects the most informative features.\n",
    "  - **PCA:** Reduces dimensionality and noise, improving model robustness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3d952",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934948ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f89ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MERGED_DIR = Path(\"merged_energy_temp\")\n",
    "files       = sorted([f for f in MERGED_DIR.glob(\"*_merged.csv\")])\n",
    "assert files, f\"No *_merged.csv found in {MERGED_DIR.resolve()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5f5e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add lagged temps, lagged energy, calendar, and interaction features.\"\"\"\n",
    "    # Ensure Datetime index or column\n",
    "    if 'Datetime' in df.columns:\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "        df = df.set_index('Datetime')\n",
    "    else:\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # 1. Extract calendar features\n",
    "    df['month'] = df.index.month\n",
    "    df['dayofweek'] = df.index.dayofweek  # Monday=0, Sunday=6\n",
    "    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "    # 2. Lagged temperature features\n",
    "    \n",
    "    # a. Short lagged temperature features\n",
    "    df[\"Temp_lag1\"] = df[\"Estimated_Temp_C\"].shift(1)\n",
    "    df[\"Temp_lag2\"] = df[\"Estimated_Temp_C\"].shift(2)\n",
    "    df[\"Temp_lag3\"] = df[\"Estimated_Temp_C\"].shift(3)\n",
    "\n",
    "    # b. Longer lagged energy features\n",
    "    df['Energy_lag1'] = df['Energy'].shift(1)\n",
    "    df['Energy_lag2'] = df['Energy'].shift(2)\n",
    "    df['Energy_lag3'] = df['Energy'].shift(3)\n",
    "    df['Energy_lag7'] = df['Energy'].shift(7)\n",
    "\n",
    "    # 3. Sin/cos features for day of year\n",
    "    doy = df.index.dayofyear\n",
    "    df[\"sin_DOY\"] = np.sin(2 * np.pi * doy / 365.25)\n",
    "    df[\"cos_DOY\"] = np.cos(2 * np.pi * doy / 365.25)\n",
    "\n",
    "    # 4. Interaction features: temperature × lagged energy\n",
    "    df['Temp_x_lag1'] = df['Estimated_Temp_C'] * df['Energy_lag1']\n",
    "    df['Temp_x_lag2'] = df['Estimated_Temp_C'] * df['Energy_lag2']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d041606",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LAG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimated_Temp_C\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimated_Temp_C\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m make_features(df)\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[0;32m     14\u001b[0m feature_cols \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimated_Temp_C\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;241m*\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemp_lag\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, MAX_LAG \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnergy_lag1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnergy_lag2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnergy_lag3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnergy_lag7\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdayofweek\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_weekend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin_DOY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos_DOY\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemp_x_lag1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemp_x_lag2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m ]\n\u001b[0;32m     22\u001b[0m X \u001b[38;5;241m=\u001b[39m df[feature_cols]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnergy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MAX_LAG' is not defined"
     ]
    }
   ],
   "source": [
    "files = sorted(MERGED_DIR.glob(\"*_merged.csv\"))\n",
    "assert files, f\"No *_merged.csv found in {MERGED_DIR.resolve()}\"\n",
    "\n",
    "data_dict = {}\n",
    "for csv_path in files:\n",
    "    region = csv_path.stem.split(\"_\")[0]\n",
    "\n",
    "    df = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
    "    df = df.rename(columns={df.columns[0]: \"Energy\"})\n",
    "    df[\"Estimated_Temp_C\"] = df[\"Estimated_Temp_C\"].astype(float)\n",
    "\n",
    "    df = make_features(df).dropna().sort_index()\n",
    "\n",
    "    feature_cols = [\n",
    "        \"Estimated_Temp_C\",\n",
    "        *(f\"Temp_lag{i}\" for i in range(1, MAX_LAG + 1)),\n",
    "        \"Energy_lag1\", \"Energy_lag2\", \"Energy_lag3\", \"Energy_lag7\",\n",
    "        \"month\", \"dayofweek\", \"is_weekend\",\n",
    "        \"sin_DOY\", \"cos_DOY\",\n",
    "        \"Temp_x_lag1\", \"Temp_x_lag2\",\n",
    "    ]\n",
    "    X = df[feature_cols].values\n",
    "    y = df[\"Energy\"].values\n",
    "    split = int(len(df) * 0.80)   # 80 % train, 20 % test\n",
    "\n",
    "    data_dict[region] = {\n",
    "        \"X_train\": X[:split],  \"y_train\": y[:split],\n",
    "        \"X_test\":  X[split:],  \"y_test\":  y[split:],\n",
    "        \"index_test\": df.index[split:],\n",
    "    }\n",
    "\n",
    "print(f\"Loaded data for {len(data_dict)} regions: {list(data_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a55228",
   "metadata": {},
   "source": [
    "# ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d846f57",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">1. XGBoost ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158f0c7",
   "metadata": {},
   "source": [
    "\n",
    "The code below demonstrates how to use the XGBoost machine learning algorithm to predict energy consumption for different regions based on weather and seasonal features. XGBoost (Extreme Gradient Boosting) is a powerful and efficient implementation of gradient boosted decision trees, widely used for regression and classification tasks due to its high performance and scalability.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it trains an `XGBRegressor` model using the training data (`X_train`, `y_train`).\n",
    "- It predicts energy values on the test set (`X_test`).\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use XGBoost?**\n",
    "- XGBoost is known for its speed and performance, especially on structured/tabular data.\n",
    "- It handles missing values and supports regularization to reduce overfitting.\n",
    "- It can capture complex, non-linear relationships in the data.\n",
    "- It is robust to outliers and can handle large datasets efficiently.\n",
    "\n",
    "In this notebook, XGBoost is used to model the relationship between temperature, seasonal effects, and energy consumption, providing accurate predictions and insights into regional energy demand patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340449dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "    y_pred = model.predict(d[\"X_test\"])\n",
    "    rmse = np.sqrt(mean_squared_error(d[\"y_test\"], y_pred))\n",
    "    r2 = r2_score(d[\"y_test\"], y_pred)\n",
    "    results[region] = {\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": y_pred,\n",
    "        \"index_test\": d[\"index_test\"]\n",
    "    }\n",
    "\n",
    "# Visualize RMSE and R2 for all regions\n",
    "regions = list(results.keys())\n",
    "rmses = [results[r][\"rmse\"] for r in regions]\n",
    "r2s = [results[r][\"r2\"] for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].bar(regions, rmses)\n",
    "ax[0].set_title(\"RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "ax[1].bar(regions, r2s)\n",
    "ax[1].set_title(\"R2 Score by Region\")\n",
    "ax[1].set_ylabel(\"R2 Score\")\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Visualize predictions vs actuals for each region (first 3 regions as example)\n",
    "for region in regions[:3]:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(results[region][\"index_test\"], results[region][\"y_test\"], label=\"Actual\")\n",
    "    plt.plot(results[region][\"index_test\"], results[region][\"y_pred\"], label=\"Predicted\")\n",
    "    plt.title(f\"{region} - Actual vs Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8acb753",
   "metadata": {},
   "source": [
    "## <span style=\"color:pink\">2. SVR ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20321102",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use the Support Vector Regression (SVR) machine learning algorithm to predict energy consumption for different regions based on weather and seasonal features. SVR is a regression technique based on Support Vector Machines (SVM), which is widely used for both classification and regression tasks due to its flexibility and ability to model complex relationships.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it trains an `SVR` model using the training data (`X_train`, `y_train`).\n",
    "- It predicts energy values on the test set (`X_test`).\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use SVR?**\n",
    "- SVR is effective for capturing non-linear relationships in data by using different kernel functions (e.g., RBF, polynomial).\n",
    "- It is robust to outliers due to the use of a margin of tolerance (epsilon) around the regression line.\n",
    "- SVR can perform well on smaller datasets and is less prone to overfitting compared to some other models.\n",
    "- It is useful when the relationship between features and target is complex and not easily captured by linear models.\n",
    "\n",
    "In this notebook, SVR is used to model the relationship between temperature, seasonal effects, and energy consumption, providing an alternative approach to tree-based models like XGBoost for regional energy demand prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e19490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "svr_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    svr_model = SVR(kernel='rbf')\n",
    "    svr_model.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "    svr_y_pred = svr_model.predict(d[\"X_test\"])\n",
    "    svr_rmse = np.sqrt(mean_squared_error(d[\"y_test\"], svr_y_pred))\n",
    "    svr_r2 = r2_score(d[\"y_test\"], svr_y_pred)\n",
    "    svr_results[region] = {\n",
    "        \"rmse\": svr_rmse,\n",
    "        \"r2\": svr_r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": svr_y_pred,\n",
    "        \"index_test\": d[\"index_test\"]\n",
    "    }\n",
    "\n",
    "# Visualize SVR RMSE and R2 for all regions\n",
    "svr_rmses = [svr_results[r][\"rmse\"] for r in regions]\n",
    "svr_r2s = [svr_results[r][\"r2\"] for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].bar(regions, svr_rmses)\n",
    "ax[0].set_title(\"SVR RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "ax[1].bar(regions, svr_r2s)\n",
    "ax[1].set_title(\"SVR R2 Score by Region\")\n",
    "ax[1].set_ylabel(\"R2 Score\")\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize SVR predictions vs actuals for each region (first 3 regions as example)\n",
    "for region in regions[:3]:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(svr_results[region][\"index_test\"], svr_results[region][\"y_test\"], label=\"Actual\")\n",
    "    plt.plot(svr_results[region][\"index_test\"], svr_results[region][\"y_pred\"], label=\"SVR Predicted\")\n",
    "    plt.title(f\"{region} - Actual vs SVR Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db990070",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">3. Random Forest ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295f02d8",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use the Random Forest (RF) machine learning algorithm to predict energy consumption for different regions based on weather and seasonal features. Random Forest is an ensemble learning method that builds multiple decision trees and combines their outputs to improve predictive accuracy and control overfitting.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it trains a `RandomForestRegressor` model using the training data (`X_train`, `y_train`).\n",
    "- It predicts energy values on the test set (`X_test`).\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use Random Forest?**\n",
    "- Random Forest is robust to overfitting and can handle high-dimensional data well.\n",
    "- It can model complex, non-linear relationships between features and the target variable.\n",
    "- It is less sensitive to outliers and noise compared to individual decision trees.\n",
    "- Random Forest provides feature importance scores, which can help in understanding which variables are most influential in the predictions.\n",
    "\n",
    "In this notebook, Random Forest is used to model the relationship between temperature, seasonal effects, and energy consumption, offering a strong baseline and interpretability for regional energy demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef905701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "rf_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "    rf_y_pred = rf_model.predict(d[\"X_test\"])\n",
    "    rf_rmse = np.sqrt(mean_squared_error(d[\"y_test\"], rf_y_pred))\n",
    "    rf_r2 = r2_score(d[\"y_test\"], rf_y_pred)\n",
    "    rf_results[region] = {\n",
    "        \"rmse\": rf_rmse,\n",
    "        \"r2\": rf_r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": rf_y_pred,\n",
    "        \"index_test\": d[\"index_test\"]\n",
    "    }\n",
    "\n",
    "# Visualize Random Forest RMSE and R2 for all regions\n",
    "rf_rmses = [rf_results[r][\"rmse\"] for r in regions]\n",
    "rf_r2s = [rf_results[r][\"r2\"] for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].bar(regions, rf_rmses)\n",
    "ax[0].set_title(\"Random Forest RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "ax[1].bar(regions, rf_r2s)\n",
    "ax[1].set_title(\"Random Forest R2 Score by Region\")\n",
    "ax[1].set_ylabel(\"R2 Score\")\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize Random Forest predictions vs actuals for each region (first 3 regions as example)\n",
    "for region in regions[:3]:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(rf_results[region][\"index_test\"], rf_results[region][\"y_test\"], label=\"Actual\")\n",
    "    plt.plot(rf_results[region][\"index_test\"], rf_results[region][\"y_pred\"], label=\"RF Predicted\")\n",
    "    plt.title(f\"{region} - Actual vs Random Forest Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b856c",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">4. Linear Regression ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efad8ff8",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use the Linear Regression (LR) machine learning algorithm to predict energy consumption for different regions based on weather and seasonal features. Linear Regression is a fundamental and widely used regression technique that models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it trains a `LinearRegression` model using the training data (`X_train`, `y_train`).\n",
    "- It predicts energy values on the test set (`X_test`).\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use Linear Regression?**\n",
    "- Linear Regression is simple, interpretable, and computationally efficient.\n",
    "- It provides a baseline for regression tasks and helps in understanding the linear relationships between features and the target variable.\n",
    "- It is less prone to overfitting when the number of features is not too large and the relationship is approximately linear.\n",
    "- Coefficients from the model can be used to interpret the influence of each feature on energy consumption.\n",
    "\n",
    "In this notebook, Linear Regression is used to model the relationship between temperature, seasonal effects, and energy consumption, providing a straightforward and interpretable approach for regional energy demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lr_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "    lr_y_pred = lr_model.predict(d[\"X_test\"])\n",
    "    lr_rmse = np.sqrt(mean_squared_error(d[\"y_test\"], lr_y_pred))\n",
    "    lr_r2 = r2_score(d[\"y_test\"], lr_y_pred)\n",
    "    lr_results[region] = {\n",
    "        \"rmse\": lr_rmse,\n",
    "        \"r2\": lr_r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": lr_y_pred,\n",
    "        \"index_test\": d[\"index_test\"]\n",
    "    }\n",
    "\n",
    "# Visualize Linear Regression RMSE and R2 for all regions\n",
    "lr_rmses = [lr_results[r][\"rmse\"] for r in regions]\n",
    "lr_r2s = [lr_results[r][\"r2\"] for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].bar(regions, lr_rmses)\n",
    "ax[0].set_title(\"Linear Regression RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "ax[1].bar(regions, lr_r2s)\n",
    "ax[1].set_title(\"Linear Regression R2 Score by Region\")\n",
    "ax[1].set_ylabel(\"R2 Score\")\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize Linear Regression predictions vs actuals for each region (first 3 regions as example)\n",
    "for region in regions[:3]:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(lr_results[region][\"index_test\"], lr_results[region][\"y_test\"], label=\"Actual\")\n",
    "    plt.plot(lr_results[region][\"index_test\"], lr_results[region][\"y_pred\"], label=\"LR Predicted\")\n",
    "    plt.title(f\"{region} - Actual vs Linear Regression Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98cc82",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">5. LightGBM ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62afea2b",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use the LightGBM (Light Gradient Boosting Machine) machine learning algorithm to predict energy consumption for different regions based on weather and seasonal features. LightGBM is a highly efficient and fast gradient boosting framework that uses tree-based learning algorithms, making it popular for large-scale and high-performance machine learning tasks.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it trains a `LGBMRegressor` model using the training data (`X_train`, `y_train`).\n",
    "- It predicts energy values on the test set (`X_test`).\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use LightGBM?**\n",
    "- LightGBM is optimized for speed and memory efficiency, making it suitable for large datasets.\n",
    "- It supports parallel and GPU learning, which accelerates training.\n",
    "- It can handle categorical features directly and is robust to overfitting.\n",
    "- LightGBM can capture complex, non-linear relationships in the data and often achieves high predictive accuracy.\n",
    "\n",
    "In this notebook, LightGBM is used to model the relationship between temperature, seasonal effects, and energy consumption, providing fast and accurate predictions for regional energy demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lgbm_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    lgbm_model = lgb.LGBMRegressor(n_estimators=100, random_state=42)\n",
    "    lgbm_model.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "    lgbm_y_pred = lgbm_model.predict(d[\"X_test\"])\n",
    "    lgbm_rmse = np.sqrt(mean_squared_error(d[\"y_test\"], lgbm_y_pred))\n",
    "    lgbm_r2 = r2_score(d[\"y_test\"], lgbm_y_pred)\n",
    "    lgbm_results[region] = {\n",
    "        \"rmse\": lgbm_rmse,\n",
    "        \"r2\": lgbm_r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": lgbm_y_pred,\n",
    "        \"index_test\": d[\"index_test\"]\n",
    "    }\n",
    "\n",
    "# Visualize LightGBM RMSE and R2 for all regions\n",
    "lgbm_rmses = [lgbm_results[r][\"rmse\"] for r in regions]\n",
    "lgbm_r2s = [lgbm_results[r][\"r2\"] for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].bar(regions, lgbm_rmses)\n",
    "ax[0].set_title(\"LightGBM RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "ax[1].bar(regions, lgbm_r2s)\n",
    "ax[1].set_title(\"LightGBM R2 Score by Region\")\n",
    "ax[1].set_ylabel(\"R2 Score\")\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize LightGBM predictions vs actuals for each region (first 3 regions as example)\n",
    "for region in regions[:3]:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(lgbm_results[region][\"index_test\"], lgbm_results[region][\"y_test\"], label=\"Actual\")\n",
    "    plt.plot(lgbm_results[region][\"index_test\"], lgbm_results[region][\"y_pred\"], label=\"LightGBM Predicted\")\n",
    "    plt.title(f\"{region} - Actual vs LightGBM Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663eccfd",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">6. Gradient Boosting ML model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e31469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "gb_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    gb_model.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "    gb_y_pred = gb_model.predict(d[\"X_test\"])\n",
    "    gb_rmse = np.sqrt(mean_squared_error(d[\"y_test\"], gb_y_pred))\n",
    "    gb_r2 = r2_score(d[\"y_test\"], gb_y_pred)\n",
    "    gb_results[region] = {\n",
    "        \"rmse\": gb_rmse,\n",
    "        \"r2\": gb_r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": gb_y_pred,\n",
    "        \"index_test\": d[\"index_test\"]\n",
    "    }\n",
    "\n",
    "# Visualize Gradient Boosting RMSE and R2 for all regions\n",
    "gb_rmses = [gb_results[r][\"rmse\"] for r in regions]\n",
    "gb_r2s = [gb_results[r][\"r2\"] for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].bar(regions, gb_rmses)\n",
    "ax[0].set_title(\"Gradient Boosting RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "ax[1].bar(regions, gb_r2s)\n",
    "ax[1].set_title(\"Gradient Boosting R2 Score by Region\")\n",
    "ax[1].set_ylabel(\"R2 Score\")\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize Gradient Boosting predictions vs actuals for each region (first 3 regions as example)\n",
    "for region in regions[:3]:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(gb_results[region][\"index_test\"], gb_results[region][\"y_test\"], label=\"Actual\")\n",
    "    plt.plot(gb_results[region][\"index_test\"], gb_results[region][\"y_pred\"], label=\"GB Predicted\")\n",
    "    plt.title(f\"{region} - Actual vs Gradient Boosting Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f9258",
   "metadata": {},
   "source": [
    "## <span style=\"color:pink\">7. Ridge regression ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095960e",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use the Ridge Regression (Ridge) machine learning algorithm to predict energy consumption for different regions based on weather and seasonal features. Ridge Regression is a regularized version of linear regression that adds an L2 penalty (the squared magnitude of coefficients) to the loss function, which helps to prevent overfitting and manage multicollinearity among features.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it trains a `Ridge` regression model using the training data (`X_train`, `y_train`).\n",
    "- It predicts energy values on the test set (`X_test`).\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use Ridge Regression?**\n",
    "- Ridge Regression is effective when there is multicollinearity (correlated features) in the data, as it shrinks the coefficients and reduces model variance.\n",
    "- The L2 regularization helps to prevent overfitting, especially when the number of features is large relative to the number of observations.\n",
    "- Ridge provides a balance between bias and variance, often improving generalization performance compared to standard linear regression.\n",
    "- It is simple, interpretable, and computationally efficient.\n",
    "\n",
    "In this notebook, Ridge Regression is used to model the relationship between temperature, seasonal effects, and energy consumption, providing a robust and regularized approach for regional energy demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "ridge_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    ridge_model = Ridge(alpha=1.0)\n",
    "    ridge_model.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "    ridge_y_pred = ridge_model.predict(d[\"X_test\"])\n",
    "    ridge_rmse = np.sqrt(mean_squared_error(d[\"y_test\"], ridge_y_pred))\n",
    "    ridge_r2 = r2_score(d[\"y_test\"], ridge_y_pred)\n",
    "    ridge_results[region] = {\n",
    "        \"rmse\": ridge_rmse,\n",
    "        \"r2\": ridge_r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": ridge_y_pred,\n",
    "        \"index_test\": d[\"index_test\"]\n",
    "    }\n",
    "\n",
    "# Visualize Ridge Regression RMSE and R2 for all regions\n",
    "ridge_rmses = [ridge_results[r][\"rmse\"] for r in regions]\n",
    "ridge_r2s = [ridge_results[r][\"r2\"] for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].bar(regions, ridge_rmses)\n",
    "ax[0].set_title(\"Ridge Regression RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "ax[1].bar(regions, ridge_r2s)\n",
    "ax[1].set_title(\"Ridge Regression R2 Score by Region\")\n",
    "ax[1].set_ylabel(\"R2 Score\")\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize Ridge Regression predictions vs actuals for each region (first 3 regions as example)\n",
    "for region in regions[:3]:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(ridge_results[region][\"index_test\"], ridge_results[region][\"y_test\"], label=\"Actual\")\n",
    "    plt.plot(ridge_results[region][\"index_test\"], ridge_results[region][\"y_pred\"], label=\"Ridge Predicted\")\n",
    "    plt.title(f\"{region} - Actual vs Ridge Regression Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a776e",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">8. SVR pipeline ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd03ed5",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use a machine learning pipeline that combines feature scaling, feature selection, dimensionality reduction, and Support Vector Regression (SVR) to predict energy consumption for different regions. This approach leverages the strengths of each preprocessing step to improve the performance of SVR, as evidenced by the increased accuracy compared to a plain SVR model.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it constructs a `Pipeline` consisting of:\n",
    "    - `StandardScaler`: Standardizes features by removing the mean and scaling to unit variance, which is important for SVR since it is sensitive to the scale of input features.\n",
    "    - `SelectKBest`: Selects the best features based on univariate linear regression tests, helping to remove irrelevant or less informative features.\n",
    "    - `PCA`: Applies Principal Component Analysis to reduce dimensionality while retaining 95% of the variance, which can help to denoise the data and reduce overfitting.\n",
    "    - `SVR`: Fits a Support Vector Regression model with an RBF kernel to capture non-linear relationships.\n",
    "- The pipeline is trained on the training data and evaluated on the test set.\n",
    "- Model performance is assessed using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- Results are visualized for all regions, and actual vs. predicted energy values are plotted for selected regions.\n",
    "\n",
    "**Why does this pipeline improve SVR accuracy?**\n",
    "- **Scaling** ensures all features contribute equally to the SVR model, preventing features with larger scales from dominating.\n",
    "- **Feature selection** removes noise and irrelevant features, allowing the model to focus on the most predictive variables.\n",
    "- **Dimensionality reduction (PCA)** helps to further reduce noise and multicollinearity, making the SVR model more robust and less prone to overfitting.\n",
    "- **SVR** benefits from these preprocessing steps, leading to better generalization and improved accuracy, as seen in the higher R² scores.\n",
    "\n",
    "By integrating these preprocessing steps, the pipeline addresses common challenges in regression modeling and enhances the predictive power of SVR for regional energy demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "svr_pipe_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    svr_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=f_regression, k='all')),\n",
    "        ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "        ('svr', SVR(kernel='rbf', C=10, gamma='scale')),\n",
    "    ])\n",
    "    svr_pipe.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "    svr_pipe_y_pred = svr_pipe.predict(d[\"X_test\"])\n",
    "    svr_pipe_rmse = np.sqrt(mean_squared_error(d[\"y_test\"], svr_pipe_y_pred))\n",
    "    svr_pipe_r2 = r2_score(d[\"y_test\"], svr_pipe_y_pred)\n",
    "    svr_pipe_results[region] = {\n",
    "        \"rmse\": svr_pipe_rmse,\n",
    "        \"r2\": svr_pipe_r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": svr_pipe_y_pred,\n",
    "        \"index_test\": d[\"index_test\"]\n",
    "    }\n",
    "\n",
    "# Visualize SVR Pipeline RMSE and R2 for all regions\n",
    "svr_pipe_rmses = [svr_pipe_results[r][\"rmse\"] for r in regions]\n",
    "svr_pipe_r2s = [svr_pipe_results[r][\"r2\"] for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].bar(regions, svr_pipe_rmses)\n",
    "ax[0].set_title(\"SVR Pipeline RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "ax[1].bar(regions, svr_pipe_r2s)\n",
    "ax[1].set_title(\"SVR Pipeline R2 Score by Region\")\n",
    "ax[1].set_ylabel(\"R2 Score\")\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize SVR Pipeline predictions vs actuals for each region (first 3 regions as example)\n",
    "for region in regions[:3]:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(svr_pipe_results[region][\"index_test\"], svr_pipe_results[region][\"y_test\"], label=\"Actual\")\n",
    "    plt.plot(svr_pipe_results[region][\"index_test\"], svr_pipe_results[region][\"y_pred\"], label=\"SVR Pipeline Predicted\")\n",
    "    plt.title(f\"{region} - Actual vs SVR Pipeline Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f1780",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">9. XGBoost + SVR pipeline ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26a1179",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use a stacking ensemble that combines an SVR pipeline and XGBoost to predict energy consumption for different regions based on weather and seasonal features. This approach leverages the strengths of both SVR (with preprocessing) and XGBoost by stacking their predictions, allowing a final XGBoost model to learn from both sets of outputs for improved accuracy.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it builds a `StackingRegressor` with two base estimators:\n",
    "    - An SVR pipeline that includes feature scaling, feature selection, and PCA for dimensionality reduction, followed by an SVR model.\n",
    "    - An XGBoost regressor.\n",
    "- The outputs of these base models are combined and used as input features for a final XGBoost regressor (the meta-learner).\n",
    "- The stacked model is trained on the training data and evaluated on the test set.\n",
    "- Model performance is assessed using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- Results are visualized for all regions, and actual vs. predicted energy values are plotted for selected regions.\n",
    "\n",
    "**Why does this stacking approach work?**\n",
    "- **Model diversity:** SVR and XGBoost capture different aspects of the data; SVR is effective for smooth, non-linear relationships after preprocessing, while XGBoost excels at handling complex interactions and outliers.\n",
    "- **Preprocessing pipeline:** The SVR pipeline ensures that features are well-scaled, relevant, and denoised before regression, which helps SVR perform optimally.\n",
    "- **Meta-learning:** The final XGBoost regressor (meta-learner) can learn how to best combine the predictions from both base models, correcting their individual weaknesses and leveraging their strengths.\n",
    "- **Reduced overfitting:** By combining multiple models, stacking can reduce the risk of overfitting compared to relying on a single model.\n",
    "- **Improved accuracy:** The ensemble often achieves better predictive performance than any individual model, as evidenced by higher R² scores and lower RMSE.\n",
    "\n",
    "This stacking ensemble provides a robust and flexible framework for regional energy demand forecasting, taking advantage of both advanced preprocessing and powerful machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf257e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "svr_xgb_stack_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    svr_estimator = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('feature_selection', SelectKBest(score_func=f_regression, k='all')),\n",
    "        ('pca', PCA(n_components=0.95, svd_solver='full')),\n",
    "        ('svr', SVR(kernel='rbf', C=10, gamma='scale')),\n",
    "    ])\n",
    "    xgb_estimator = XGBRegressor(n_estimators=100, random_state=42)\n",
    "    stack = StackingRegressor(\n",
    "        estimators=[\n",
    "            ('svr_pipe', svr_estimator),\n",
    "            ('xgb', xgb_estimator)\n",
    "        ],\n",
    "        final_estimator=XGBRegressor(n_estimators=50, random_state=42),\n",
    "        passthrough=True\n",
    "    )\n",
    "    stack.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "    stack_y_pred = stack.predict(d[\"X_test\"])\n",
    "    stack_rmse = np.sqrt(mean_squared_error(d[\"y_test\"], stack_y_pred))\n",
    "    stack_r2 = r2_score(d[\"y_test\"], stack_y_pred)\n",
    "    svr_xgb_stack_results[region] = {\n",
    "        \"rmse\": stack_rmse,\n",
    "        \"r2\": stack_r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": stack_y_pred,\n",
    "        \"index_test\": d[\"index_test\"]\n",
    "    }\n",
    "\n",
    "# Visualize SVR+XGBoost Stacking RMSE and R2 for all regions\n",
    "stack_rmses = [svr_xgb_stack_results[r][\"rmse\"] for r in regions]\n",
    "stack_r2s = [svr_xgb_stack_results[r][\"r2\"] for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].bar(regions, stack_rmses)\n",
    "ax[0].set_title(\"SVR+XGBoost Stacking RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "ax[1].bar(regions, stack_r2s)\n",
    "ax[1].set_title(\"SVR+XGBoost Stacking R2 Score by Region\")\n",
    "ax[1].set_ylabel(\"R2 Score\")\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize predictions vs actuals for each region (first 3 regions as example)\n",
    "for region in regions[:3]:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(svr_xgb_stack_results[region][\"index_test\"], svr_xgb_stack_results[region][\"y_test\"], label=\"Actual\")\n",
    "    plt.plot(svr_xgb_stack_results[region][\"index_test\"], svr_xgb_stack_results[region][\"y_pred\"], label=\"SVR+XGBoost Stacked Predicted\")\n",
    "    plt.title(f\"{region} - Actual vs SVR+XGBoost Stacked Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3b66c",
   "metadata": {},
   "source": [
    "## <span style=\"color:pink\">10. LSTM ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74625109",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use a Long Short-Term Memory (LSTM) neural network to predict energy consumption for different regions based on weather, seasonal, and historical energy features. LSTM is a type of recurrent neural network (RNN) that is well-suited for modeling sequential and time series data due to its ability to capture long-term dependencies and temporal patterns.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it transforms the feature and target arrays into sequences of a fixed length (`SEQ_LEN`), so the LSTM can learn from historical windows of data.\n",
    "- It builds and trains an LSTM-based neural network using the training sequences, with dropout for regularization and dense layers for additional learning capacity.\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use LSTM?**\n",
    "- LSTM networks are specifically designed to handle sequential data and can learn complex temporal relationships, making them ideal for time series forecasting.\n",
    "- They are effective at capturing both short-term and long-term dependencies in data, which is important for energy demand prediction where past consumption and weather patterns influence future values.\n",
    "- LSTMs can model non-linear and non-stationary processes, outperforming traditional models when the data exhibits complex temporal dynamics.\n",
    "- They are robust to noise and can generalize well when trained with sufficient data.\n",
    "\n",
    "In this notebook, LSTM is used to model the relationship between historical energy usage, temperature, seasonal effects, and future energy demand, providing a powerful deep learning approach for regional energy forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lstm_results = {}\n",
    "\n",
    "SEQ_LEN = 7\n",
    "\n",
    "def create_lstm_sequences(X, y, seq_len):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        Xs.append(X[i:i+seq_len])\n",
    "        ys.append(y[i+seq_len])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    X_train_seq, y_train_seq = create_lstm_sequences(d[\"X_train\"], d[\"y_train\"], SEQ_LEN)\n",
    "    X_test_seq, y_test_seq = create_lstm_sequences(d[\"X_test\"], d[\"y_test\"], SEQ_LEN)\n",
    "    index_test_seq = d[\"index_test\"][SEQ_LEN:]\n",
    "\n",
    "    # Build improved LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, activation='relu', input_shape=(SEQ_LEN, X_train_seq.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    y_pred_seq = model.predict(X_test_seq).flatten()\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred_seq))\n",
    "    r2 = r2_score(y_test_seq, y_pred_seq)\n",
    "\n",
    "    lstm_results[region] = {\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "        \"y_test\": y_test_seq,\n",
    "        \"y_pred\": y_pred_seq,\n",
    "        \"index_test\": index_test_seq\n",
    "    }\n",
    "\n",
    "# Visualize LSTM RMSE and R2 for all regions\n",
    "lstm_rmses = [lstm_results[r][\"rmse\"] for r in regions]\n",
    "lstm_r2s = [lstm_results[r][\"r2\"] for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].bar(regions, lstm_rmses)\n",
    "ax[0].set_title(\"LSTM RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "ax[1].bar(regions, lstm_r2s)\n",
    "ax[1].set_title(\"LSTM R2 Score by Region\")\n",
    "ax[1].set_ylabel(\"R2 Score\")\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize LSTM predictions vs actuals for each region (first 3 regions as example)\n",
    "for region in regions[:3]:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(lstm_results[region][\"index_test\"], lstm_results[region][\"y_test\"], label=\"Actual\")\n",
    "    plt.plot(lstm_results[region][\"index_test\"], lstm_results[region][\"y_pred\"], label=\"LSTM Predicted\")\n",
    "    plt.title(f\"{region} - Actual vs LSTM Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df7997",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare model names and their corresponding RMSE and R2 lists\n",
    "model_names = [\n",
    "    \"XGBoost\", \"SVR\", \"Random Forest\", \"Linear Regression\", \"LightGBM\",\n",
    "    \"Gradient Boosting\", \"Ridge Regression\", \"SVR Pipeline\", \"XGBoost+SVR Stack\", \"LSTM\"\n",
    "]\n",
    "\n",
    "# Gather RMSE and R2 lists for each model (order matches model_names)\n",
    "rmse_lists = [\n",
    "    rmses,                # XGBoost\n",
    "    svr_rmses,            # SVR\n",
    "    rf_rmses,             # Random Forest\n",
    "    lr_rmses,             # Linear Regression\n",
    "    lgbm_rmses,           # LightGBM\n",
    "    gb_rmses,             # Gradient Boosting\n",
    "    ridge_rmses,          # Ridge Regression\n",
    "    svr_pipe_rmses,       # SVR Pipeline\n",
    "    stack_rmses,          # XGBoost+SVR Stack\n",
    "    lstm_rmses            # LSTM\n",
    "]\n",
    "\n",
    "r2_lists = [\n",
    "    r2s,                  # XGBoost\n",
    "    svr_r2s,              # SVR\n",
    "    rf_r2s,               # Random Forest\n",
    "    lr_r2s,               # Linear Regression\n",
    "    lgbm_r2s,             # LightGBM\n",
    "    gb_r2s,               # Gradient Boosting\n",
    "    ridge_r2s,            # Ridge Regression\n",
    "    svr_pipe_r2s,         # SVR Pipeline\n",
    "    stack_r2s,            # XGBoost+SVR Stack\n",
    "    lstm_r2s              # LSTM\n",
    "]\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "compare_df = pd.DataFrame({\n",
    "    \"Model\": model_names_short + [\"LSTM\"],\n",
    "    \"Train RMSE\": train_rmse_list + [np.nan],\n",
    "    \"Val RMSE\": val_rmse_list + [lstm_val_rmse],\n",
    "    \"Train R2\": train_r2_list + [np.nan],\n",
    "    \"Val R2\": val_r2_list + [lstm_val_r2]\n",
    "})\n",
    "\n",
    "compare_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b3ca2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c6164",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics: RMSE and R²\n",
    "\n",
    "When comparing machine learning models for energy prediction, two key metrics are used:\n",
    "\n",
    "### 1. Root Mean Squared Error (RMSE)\n",
    "- **Definition:** RMSE measures the average magnitude of the errors between predicted and actual values. It is the square root of the average of squared differences.\n",
    "- **Interpretation:**  \n",
    "    - **Low RMSE:** Indicates that the model's predictions are close to the actual values, meaning higher accuracy.\n",
    "    - **High RMSE:** Indicates larger errors in predictions, meaning lower accuracy.\n",
    "- **Units:** RMSE is in the same units as the target variable (here, energy).\n",
    "\n",
    "### 2. R² Score (Coefficient of Determination)\n",
    "- **Definition:** R² measures the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "- **Interpretation:**  \n",
    "    - **R² = 1:** Perfect prediction.\n",
    "    - **R² = 0:** Model predicts no better than the mean of the data.\n",
    "    - **R² < 0:** Model performs worse than simply predicting the mean.\n",
    "    - **Higher R²:** Indicates better model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparing Models\n",
    "\n",
    "- **Best Models:**  \n",
    "    - Models with **lowest RMSE** and **highest R²** are considered best, as they make predictions closest to the actual values and explain the most variance.\n",
    "    - In the results above, ensemble models like **XGBoost+SVR Stacking** and advanced pipelines (e.g., SVR Pipeline) generally achieve lower RMSE and higher R² across regions, indicating superior performance.\n",
    "    - Simpler models (e.g., plain SVR or Linear Regression) tend to have higher RMSE and lower R², showing less accurate predictions.\n",
    "\n",
    "- **Why Some Models Perform Better:**  \n",
    "    - **Ensemble and pipeline models** combine the strengths of multiple algorithms and preprocessing steps, capturing complex relationships and reducing overfitting.\n",
    "    - **Tree-based models** (like XGBoost, Random Forest, LightGBM) handle non-linearities and interactions well, which are common in energy data.\n",
    "    - **Regularized models** (like Ridge Regression) help when features are correlated, but may not capture complex patterns as well as ensembles.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "- **Low RMSE and high R²** indicate a model is making accurate and reliable predictions.\n",
    "- **Model selection** should be based on these metrics, with preference for models that consistently perform well across all regions.\n",
    "- The results show that **stacking ensembles and advanced pipelines** provide the best predictive performance for regional energy forecasting in this workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
