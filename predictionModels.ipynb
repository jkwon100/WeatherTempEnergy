{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68d02cc",
   "metadata": {},
   "source": [
    "# Energy Prediction Notebook\n",
    "\n",
    "This notebook demonstrates a comprehensive workflow for forecasting regional energy demand using a variety of machine learning models. The workflow includes robust feature engineering, model training, evaluation, and comparison across multiple advanced regression algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## **Workflow Overview**\n",
    "\n",
    "1. **Data Preparation**\n",
    "    - Regional energy consumption and weather data are loaded and merged.\n",
    "    - Data is split into training and test sets for each region.\n",
    "\n",
    "2. **Feature Engineering**\n",
    "    - **Lagged Features:** Past values of temperature and energy are added as features to capture temporal dependencies.\n",
    "    - **Calendar Features:** Month, day of week, and weekend indicators are extracted to model seasonality and weekly patterns.\n",
    "    - **Sin/Cos Seasonality:** Sine and cosine transformations of the day of year are used to encode annual seasonality in a continuous, cyclical manner.\n",
    "    - **Interaction Features:** Multiplicative interactions between temperature and lagged energy are included to capture non-linear effects.\n",
    "    - **Dimensionality Reduction & Selection (in pipelines):** Feature selection (SelectKBest) and Principal Component Analysis (PCA) are used in some pipelines to reduce noise and multicollinearity.\n",
    "\n",
    "3. **Model Training & Evaluation**\n",
    "    - Multiple machine learning models are trained for each region.\n",
    "    - Performance is evaluated using RMSE (Root Mean Squared Error) and R² (coefficient of determination).\n",
    "    - Results are visualized and compared across models and regions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Models Implemented**\n",
    "\n",
    "1. **XGBoost Regressor**\n",
    "    - A powerful gradient boosting tree model, robust to outliers and capable of capturing complex relationships.\n",
    "\n",
    "2. **Support Vector Regression (SVR)**\n",
    "    - A kernel-based regression model effective for non-linear relationships.\n",
    "\n",
    "3. **Random Forest Regressor**\n",
    "    - An ensemble of decision trees that reduces overfitting and improves generalization.\n",
    "\n",
    "4. **Linear Regression**\n",
    "    - A simple, interpretable baseline model for linear relationships.\n",
    "\n",
    "5. **LightGBM Regressor**\n",
    "    - A fast, efficient gradient boosting framework optimized for large datasets.\n",
    "\n",
    "6. **Gradient Boosting Regressor**\n",
    "    - Another tree-based boosting model, similar to XGBoost but from scikit-learn.\n",
    "\n",
    "7. **Ridge Regression**\n",
    "    - Linear regression with L2 regularization to handle multicollinearity and prevent overfitting.\n",
    "\n",
    "8. **SVR Pipeline**\n",
    "    - A pipeline combining scaling, feature selection, PCA, and SVR to improve performance by preprocessing features.\n",
    "\n",
    "9. **XGBoost + SVR Stacking Ensemble**\n",
    "    - A stacking regressor that combines predictions from an SVR pipeline and XGBoost, using another XGBoost as a meta-learner for improved accuracy.\n",
    "\n",
    "10. **LSTM Neural Network**\n",
    "     - A deep learning model (Long Short-Term Memory) designed for sequential data, capturing long-term temporal dependencies in energy demand.\n",
    "\n",
    "---\n",
    "\n",
    "## **Feature Engineering**\n",
    "\n",
    "- **Lagged Features:**  \n",
    "  - `Temp_lag1`, `Temp_lag2`, `Temp_lag3`: Previous days' temperatures.\n",
    "  - `Energy_lag1`, `Energy_lag2`, `Energy_lag3`, `Energy_lag7`: Previous days' energy consumption.\n",
    "\n",
    "- **Calendar Features:**  \n",
    "  - `month`, `dayofweek`, `is_weekend`: Encodes seasonality and weekly cycles.\n",
    "\n",
    "- **Seasonality (Sin/Cos):**  \n",
    "  - `sin_DOY`, `cos_DOY`: Encodes cyclical annual patterns.\n",
    "\n",
    "- **Interaction Features:**  \n",
    "  - `Temp_x_lag1`, `Temp_x_lag2`: Captures non-linear effects between temperature and past energy.\n",
    "\n",
    "- **Advanced Pipelines:**  \n",
    "  - **Scaling:** Standardizes features for models sensitive to feature scale (e.g., SVR).\n",
    "  - **Feature Selection:** Selects the most informative features.\n",
    "  - **PCA:** Reduces dimensionality and noise, improving model robustness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3d952",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934948ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "numpy._core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m__init__.cython-30.pxd:1024\u001b[0m, in \u001b[0;36mnumpy.import_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression, Ridge\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVR\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\__init__.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ARDRegression, BayesianRidge\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_coordinate_descent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     ElasticNet,\n\u001b[0;32m     14\u001b[0m     ElasticNetCV,\n\u001b[0;32m     15\u001b[0m     Lasso,\n\u001b[0;32m     16\u001b[0m     LassoCV,\n\u001b[0;32m     17\u001b[0m     MultiTaskElasticNet,\n\u001b[0;32m     18\u001b[0m     MultiTaskElasticNetCV,\n\u001b[0;32m     19\u001b[0m     MultiTaskLasso,\n\u001b[0;32m     20\u001b[0m     MultiTaskLassoCV,\n\u001b[0;32m     21\u001b[0m     enet_path,\n\u001b[0;32m     22\u001b[0m     lasso_path,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_glm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GammaRegressor, PoissonRegressor, TweedieRegressor\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_huber\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuberRegressor\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiOutputMixin, RegressorMixin, _fit_context\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_cv\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch, check_array, check_scalar\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     MetadataRouter,\n\u001b[0;32m     22\u001b[0m     MethodMapping,\n\u001b[0;32m     23\u001b[0m     _raise_for_params,\n\u001b[0;32m     24\u001b[0m     get_routing_for_object,\n\u001b[0;32m     25\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Authors: The scikit-learn developers\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification_threshold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     FixedThresholdClassifier,\n\u001b[0;32m     10\u001b[0m     TunedThresholdClassifierCV,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningCurveDisplay, ValidationCurveDisplay\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV, ParameterGrid, ParameterSampler, RandomizedSearchCV\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_classification_threshold.py:17\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     BaseEstimator,\n\u001b[0;32m     11\u001b[0m     ClassifierMixin,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     clone,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotFittedError\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     check_scoring,\n\u001b[0;32m     19\u001b[0m     get_scorer_names,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_scorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     _CurveScorer,\n\u001b[0;32m     23\u001b[0m     _threshold_scores_to_class_labels,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing, get_tags\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Score functions, performance metrics, pairwise metrics and distance computations.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Authors: The scikit-learn developers\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     accuracy_score,\n\u001b[0;32m      9\u001b[0m     balanced_accuracy_score,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     zero_one_loss,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistanceMetric\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\cluster\\__init__.py:28\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bicluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m consensus_score\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_supervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     adjusted_mutual_info_score,\n\u001b[0;32m     14\u001b[0m     adjusted_rand_score,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     v_measure_score,\n\u001b[0;32m     27\u001b[0m )\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_unsupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     calinski_harabasz_score,\n\u001b[0;32m     30\u001b[0m     davies_bouldin_score,\n\u001b[0;32m     31\u001b[0m     silhouette_samples,\n\u001b[0;32m     32\u001b[0m     silhouette_score,\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     35\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjusted_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsensus_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _atol_for_type\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     Interval,\n\u001b[0;32m     18\u001b[0m     StrOptions,\n\u001b[0;32m     19\u001b[0m     validate_params,\n\u001b[0;32m     20\u001b[0m )\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VALID_METRICS, pairwise_distances, pairwise_distances_chunked\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_number_of_labels\u001b[39m(n_labels, n_samples):\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m        Number of samples.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _num_samples, check_non_negative\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_distances_reduction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArgKmin\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _chi2_kernel_fast, _sparse_manhattan\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Utility Functions\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\__init__.py:97\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Authors: The scikit-learn developers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# SPDX-License-Identifier: BSD-3-Clause\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#    (see :class:`MiddleTermComputer{32,64}`).\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     98\u001b[0m     ArgKmin,\n\u001b[0;32m     99\u001b[0m     ArgKminClassMode,\n\u001b[0;32m    100\u001b[0m     BaseDistancesReductionDispatcher,\n\u001b[0;32m    101\u001b[0m     RadiusNeighbors,\n\u001b[0;32m    102\u001b[0m     RadiusNeighborsClassMode,\n\u001b[0;32m    103\u001b[0m     sqeuclidean_row_norms,\n\u001b[0;32m    104\u001b[0m )\n\u001b[0;32m    106\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseDistancesReductionDispatcher\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgKmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqeuclidean_row_norms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    113\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     BOOL_METRICS,\n\u001b[0;32m     13\u001b[0m     METRIC_MAPPING64,\n\u001b[0;32m     14\u001b[0m     DistanceMetric,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_argkmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     ArgKmin32,\n\u001b[0;32m     18\u001b[0m     ArgKmin64,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_argkmin_classmode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     ArgKminClassMode32,\n\u001b[0;32m     22\u001b[0m     ArgKminClassMode64,\n\u001b[0;32m     23\u001b[0m )\n",
      "File \u001b[1;32msklearn\\\\metrics\\\\_dist_metrics.pyx:8\u001b[0m, in \u001b[0;36minit sklearn.metrics._dist_metrics\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m__init__.cython-30.pxd:1026\u001b[0m, in \u001b[0;36mnumpy.import_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy._core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGED_DIR = Path(\"merged_energy_temp\")\n",
    "MAX_LAG    = 3                            # 1- to 3-day temp lags\n",
    "TEST_SPLIT = 0.80                         # 80 % train, 20 % test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d041606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add calendar, lagged temp / energy, sinusoidal seasonality,\n",
    "    and simple interaction features.\n",
    "    \"\"\"\n",
    "    # Ensure a DatetimeIndex\n",
    "    if \"Datetime\" in df.columns:\n",
    "        df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"])\n",
    "        df = df.set_index(\"Datetime\")\n",
    "    else:\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Calendar\n",
    "    df[\"month\"]      = df.index.month\n",
    "    df[\"dayofweek\"]  = df.index.dayofweek          # Mon=0 … Sun=6\n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(int)\n",
    "\n",
    "    # Temperature lags\n",
    "    for lag in range(1, MAX_LAG + 1):\n",
    "        df[f\"Temp_lag{lag}\"] = df[\"Estimated_Temp_C\"].shift(lag)\n",
    "\n",
    "    # Energy lags\n",
    "    for lag in [1, 2, 3, 7]:\n",
    "        df[f\"Energy_lag{lag}\"] = df[\"Energy\"].shift(lag)\n",
    "\n",
    "    # Day-of-year sinusoid\n",
    "    doy = df.index.dayofyear\n",
    "    df[\"sin_DOY\"] = np.sin(2 * np.pi * doy / 365.25)\n",
    "    df[\"cos_DOY\"] = np.cos(2 * np.pi * doy / 365.25)\n",
    "\n",
    "    # Simple interaction\n",
    "    df[\"Temp_x_lag1\"] = df[\"Estimated_Temp_C\"] * df[\"Energy_lag1\"]\n",
    "    df[\"Temp_x_lag2\"] = df[\"Estimated_Temp_C\"] * df[\"Energy_lag2\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"Estimated_Temp_C\",\n",
    "    *(f\"Temp_lag{i}\" for i in range(1, MAX_LAG + 1)),\n",
    "    \"Energy_lag1\", \"Energy_lag2\", \"Energy_lag3\", \"Energy_lag7\",\n",
    "    \"month\", \"dayofweek\", \"is_weekend\",\n",
    "    \"sin_DOY\", \"cos_DOY\",\n",
    "    \"Temp_x_lag1\", \"Temp_x_lag2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7780ee-8790-491c-9858-3449909cbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(MERGED_DIR.glob(\"*_merged.csv\"))\n",
    "assert files, f\"No *_merged.csv found in {MERGED_DIR.resolve()}\"\n",
    "\n",
    "data_dict = {}\n",
    "for csv_path in files:\n",
    "    region = csv_path.stem.split(\"_\")[0]                      # e.g. \"TX\" from \"TX_merged.csv\"\n",
    "    df = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
    "\n",
    "    # First column is energy usage; rename for clarity\n",
    "    df = df.rename(columns={df.columns[0]: \"Energy\"})\n",
    "    df[\"Estimated_Temp_C\"] = df[\"Estimated_Temp_C\"].astype(float)\n",
    "\n",
    "    df = make_features(df).dropna().sort_index()\n",
    "\n",
    "    X = df[FEATURE_COLS].values\n",
    "    y = df[\"Energy\"].values\n",
    "    split = int(len(df) * TEST_SPLIT)\n",
    "\n",
    "    data_dict[region] = {\n",
    "        \"X_train\": X[:split],  \"y_train\": y[:split],\n",
    "        \"X_test\":  X[split:],  \"y_test\":  y[split:],\n",
    "        \"index_test\": df.index[split:],\n",
    "    }\n",
    "\n",
    "print(f\"Loaded {len(data_dict)} regions: {list(data_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a55228",
   "metadata": {},
   "source": [
    "# ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d846f57",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">1. XGBoost ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158f0c7",
   "metadata": {},
   "source": [
    "\n",
    "The code below demonstrates how to use the XGBoost machine learning algorithm to predict energy consumption for different regions based on weather and seasonal features. XGBoost (Extreme Gradient Boosting) is a powerful and efficient implementation of gradient boosted decision trees, widely used for regression and classification tasks due to its high performance and scalability.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it trains an `XGBRegressor` model using the training data (`X_train`, `y_train`).\n",
    "- It predicts energy values on the test set (`X_test`).\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use XGBoost?**\n",
    "- XGBoost is known for its speed and performance, especially on structured/tabular data.\n",
    "- It handles missing values and supports regularization to reduce overfitting.\n",
    "- It can capture complex, non-linear relationships in the data.\n",
    "- It is robust to outliers and can handle large datasets efficiently.\n",
    "\n",
    "In this notebook, XGBoost is used to model the relationship between temperature, seasonal effects, and energy consumption, providing accurate predictions and insights into regional energy demand patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340449dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb          # add more models if desired\n",
    "\n",
    "def train_xgb(X_tr, y_tr):\n",
    "    \"\"\"Simple XGBRegressor with sane defaults.\"\"\"\n",
    "    return xgb.XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.06,\n",
    "        max_depth=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    ).fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80adea91-3f03-4b1a-90cf-06719d7a24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    model = train_xgb(d[\"X_train\"], d[\"y_train\"])\n",
    "    preds = model.predict(d[\"X_test\"])\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(d[\"y_test\"], preds))\n",
    "    r2   = r2_score(d[\"y_test\"], preds)\n",
    "\n",
    "    results[region] = {\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": preds,\n",
    "        \"index_test\": d[\"index_test\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd9f8e-e81c-471c-a100-9f63365780f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = list(results)\n",
    "rmses   = [results[r][\"rmse\"] for r in regions]\n",
    "r2s     = [results[r][\"r2\"]   for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# left chart\n",
    "ax[0].bar(regions, rmses)\n",
    "ax[0].set_title(\"RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticks(np.arange(len(regions)))\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "# right chart\n",
    "ax[1].bar(regions, r2s)\n",
    "ax[1].set_title(\"R² by Region\")\n",
    "ax[1].set_ylabel(\"R²\")\n",
    "ax[1].set_xticks(np.arange(len(regions)))\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5867de-0a97-422e-b634-0ce678e51351",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions[:3]:\n",
    "    idx = results[region][\"index_test\"]\n",
    "    yt  = results[region][\"y_test\"]\n",
    "    yp  = results[region][\"y_pred\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(idx, yt,  label=\"Actual\")\n",
    "    plt.plot(idx, yp, \"--\", label=\"XGB Predicted\")\n",
    "    plt.title(f\"{region} – Energy: truth vs forecast\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8acb753",
   "metadata": {},
   "source": [
    "## <span style=\"color:pink\">2. SVR ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20321102",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use the Support Vector Regression (SVR) machine learning algorithm to predict energy consumption for different regions based on weather and seasonal features. SVR is a regression technique based on Support Vector Machines (SVM), which is widely used for both classification and regression tasks due to its flexibility and ability to model complex relationships.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it trains an `SVR` model using the training data (`X_train`, `y_train`).\n",
    "- It predicts energy values on the test set (`X_test`).\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use SVR?**\n",
    "- SVR is effective for capturing non-linear relationships in data by using different kernel functions (e.g., RBF, polynomial).\n",
    "- It is robust to outliers due to the use of a margin of tolerance (epsilon) around the regression line.\n",
    "- SVR can perform well on smaller datasets and is less prone to overfitting compared to some other models.\n",
    "- It is useful when the relationship between features and target is complex and not easily captured by linear models.\n",
    "\n",
    "In this notebook, SVR is used to model the relationship between temperature, seasonal effects, and energy consumption, providing an alternative approach to tree-based models like XGBoost for regional energy demand prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e19490",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    svr_model = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SVR(kernel=\"rbf\", C=10, gamma=\"scale\", epsilon=0.1)\n",
    "    )\n",
    "    svr_model.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "\n",
    "    y_pred = svr_model.predict(d[\"X_test\"])\n",
    "    rmse   = np.sqrt(mean_squared_error(d[\"y_test\"], y_pred))\n",
    "    r2     = r2_score(d[\"y_test\"], y_pred)\n",
    "\n",
    "    svr_results[region] = {\n",
    "        \"rmse\":  rmse,\n",
    "        \"r2\":    r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": y_pred,\n",
    "        \"index_test\": d[\"index_test\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3220e5-b094-4a02-ad7f-1af7c10732c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions   = list(svr_results)\n",
    "svr_rmses = [svr_results[r][\"rmse\"] for r in regions]\n",
    "svr_r2s   = [svr_results[r][\"r2\"]   for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "ax[0].bar(regions, svr_rmses)\n",
    "ax[0].set_title(\"SVR – RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticks(np.arange(len(regions)))                # set first …\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha=\"right\")  # … then labels\n",
    "\n",
    "# R²\n",
    "ax[1].bar(regions, svr_r2s)\n",
    "ax[1].set_title(\"SVR – R² by Region\")\n",
    "ax[1].set_ylabel(\"R²\")\n",
    "ax[1].set_xticks(np.arange(len(regions)))\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81f262-0f23-419e-9e4e-6e8dd1e22998",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions[:3]:\n",
    "    idx = svr_results[region][\"index_test\"]\n",
    "    yt  = svr_results[region][\"y_test\"]\n",
    "    yp  = svr_results[region][\"y_pred\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(idx, yt, label=\"Actual\")\n",
    "    plt.plot(idx, yp, label=\"SVR Predicted\")\n",
    "    plt.title(f\"{region} – Actual vs SVR Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db990070",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">3. Random Forest ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295f02d8",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use the Random Forest (RF) machine learning algorithm to predict energy consumption for different regions based on weather and seasonal features. Random Forest is an ensemble learning method that builds multiple decision trees and combines their outputs to improve predictive accuracy and control overfitting.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it trains a `RandomForestRegressor` model using the training data (`X_train`, `y_train`).\n",
    "- It predicts energy values on the test set (`X_test`).\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use Random Forest?**\n",
    "- Random Forest is robust to overfitting and can handle high-dimensional data well.\n",
    "- It can model complex, non-linear relationships between features and the target variable.\n",
    "- It is less sensitive to outliers and noise compared to individual decision trees.\n",
    "- Random Forest provides feature importance scores, which can help in understanding which variables are most influential in the predictions.\n",
    "\n",
    "In this notebook, Random Forest is used to model the relationship between temperature, seasonal effects, and energy consumption, offering a strong baseline and interpretability for regional energy demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef905701",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=300,        # more trees → stabler scores\n",
    "        max_depth=None,\n",
    "        n_jobs=-1,               # use all CPUs\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_model.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "\n",
    "    y_pred = rf_model.predict(d[\"X_test\"])\n",
    "    rmse   = np.sqrt(mean_squared_error(d[\"y_test\"], y_pred))\n",
    "    r2     = r2_score(d[\"y_test\"], y_pred)\n",
    "\n",
    "    rf_results[region] = {\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\":   r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": y_pred,\n",
    "        \"index_test\": d[\"index_test\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f4884-b372-4bf6-968d-1d9e4bf347a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions  = list(rf_results)\n",
    "rf_rmses = [rf_results[r][\"rmse\"] for r in regions]\n",
    "rf_r2s   = [rf_results[r][\"r2\"]   for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "ax[0].bar(regions, rf_rmses)\n",
    "ax[0].set_title(\"Random Forest – RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticks(np.arange(len(regions)))\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha=\"right\")\n",
    "\n",
    "# R²\n",
    "ax[1].bar(regions, rf_r2s)\n",
    "ax[1].set_title(\"Random Forest – R² by Region\")\n",
    "ax[1].set_ylabel(\"R²\")\n",
    "ax[1].set_xticks(np.arange(len(regions)))\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127621a-eb55-4b93-92dd-6a261e293523",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions[:3]:\n",
    "    idx = rf_results[region][\"index_test\"]\n",
    "    yt  = rf_results[region][\"y_test\"]\n",
    "    yp  = rf_results[region][\"y_pred\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(idx, yt, label=\"Actual\")\n",
    "    plt.plot(idx, yp, label=\"RF Predicted\")\n",
    "    plt.title(f\"{region} – Actual vs Random Forest Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b856c",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">4. Linear Regression ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efad8ff8",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use the Linear Regression (LR) machine learning algorithm to predict energy consumption for different regions based on weather and seasonal features. Linear Regression is a fundamental and widely used regression technique that models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it trains a `LinearRegression` model using the training data (`X_train`, `y_train`).\n",
    "- It predicts energy values on the test set (`X_test`).\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use Linear Regression?**\n",
    "- Linear Regression is simple, interpretable, and computationally efficient.\n",
    "- It provides a baseline for regression tasks and helps in understanding the linear relationships between features and the target variable.\n",
    "- It is less prone to overfitting when the number of features is not too large and the relationship is approximately linear.\n",
    "- Coefficients from the model can be used to interpret the influence of each feature on energy consumption.\n",
    "\n",
    "In this notebook, Linear Regression is used to model the relationship between temperature, seasonal effects, and energy consumption, providing a straightforward and interpretable approach for regional energy demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    lr_model = make_pipeline(\n",
    "        StandardScaler(),        # centre / scale features\n",
    "        LinearRegression()\n",
    "    )\n",
    "    lr_model.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "\n",
    "    y_pred = lr_model.predict(d[\"X_test\"])\n",
    "    rmse   = np.sqrt(mean_squared_error(d[\"y_test\"], y_pred))\n",
    "    r2     = r2_score(d[\"y_test\"], y_pred)\n",
    "\n",
    "    lr_results[region] = {\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\":   r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": y_pred,\n",
    "        \"index_test\": d[\"index_test\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f748a-b047-4029-8a6f-3274b844fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = list(lr_results)\n",
    "lr_rmses = [lr_results[r][\"rmse\"] for r in regions]\n",
    "lr_r2s   = [lr_results[r][\"r2\"]   for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "ax[0].bar(regions, lr_rmses)\n",
    "ax[0].set_title(\"Linear Regression – RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticks(np.arange(len(regions)))\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha=\"right\")\n",
    "\n",
    "# R²\n",
    "ax[1].bar(regions, lr_r2s)\n",
    "ax[1].set_title(\"Linear Regression – R² by Region\")\n",
    "ax[1].set_ylabel(\"R²\")\n",
    "ax[1].set_xticks(np.arange(len(regions)))\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a726c9-ace1-4dcc-99c5-8b787cea275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions[:3]:\n",
    "    idx = lr_results[region][\"index_test\"]\n",
    "    yt  = lr_results[region][\"y_test\"]\n",
    "    yp  = lr_results[region][\"y_pred\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(idx, yt, label=\"Actual\")\n",
    "    plt.plot(idx, yp, label=\"LR Predicted\")\n",
    "    plt.title(f\"{region} – Actual vs Linear Regression Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98cc82",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">5. LightGBM ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62afea2b",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use the LightGBM (Light Gradient Boosting Machine) machine learning algorithm to predict energy consumption for different regions based on weather and seasonal features. LightGBM is a highly efficient and fast gradient boosting framework that uses tree-based learning algorithms, making it popular for large-scale and high-performance machine learning tasks.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it trains a `LGBMRegressor` model using the training data (`X_train`, `y_train`).\n",
    "- It predicts energy values on the test set (`X_test`).\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use LightGBM?**\n",
    "- LightGBM is optimized for speed and memory efficiency, making it suitable for large datasets.\n",
    "- It supports parallel and GPU learning, which accelerates training.\n",
    "- It can handle categorical features directly and is robust to overfitting.\n",
    "- LightGBM can capture complex, non-linear relationships in the data and often achieves high predictive accuracy.\n",
    "\n",
    "In this notebook, LightGBM is used to model the relationship between temperature, seasonal effects, and energy consumption, providing fast and accurate predictions for regional energy demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    lgbm_model = lgb.LGBMRegressor(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,              # full depth; use a value (e.g. 6) to cap\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective=\"regression\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=-1                 # silence stdout\n",
    "    )\n",
    "\n",
    "    lgbm_model.fit(\n",
    "        d[\"X_train\"], d[\"y_train\"],\n",
    "        callbacks=[lgb.log_evaluation(period=0)]   # silence eval logging\n",
    "    )\n",
    "\n",
    "    y_pred = lgbm_model.predict(d[\"X_test\"])\n",
    "    rmse   = np.sqrt(mean_squared_error(d[\"y_test\"], y_pred))\n",
    "    r2     = r2_score(d[\"y_test\"], y_pred)\n",
    "\n",
    "    lgbm_results[region] = {\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\":   r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": y_pred,\n",
    "        \"index_test\": d[\"index_test\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c32ba8-a709-470c-981b-0699581b10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions   = list(lgbm_results)          # derive directly from results\n",
    "lgbm_rmses = [lgbm_results[r][\"rmse\"] for r in regions]\n",
    "lgbm_r2s   = [lgbm_results[r][\"r2\"]   for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "ax[0].bar(regions, lgbm_rmses)\n",
    "ax[0].set_title(\"LightGBM – RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticks(np.arange(len(regions)))\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "# R²\n",
    "ax[1].bar(regions, lgbm_r2s)\n",
    "ax[1].set_title(\"LightGBM – R² by Region\")\n",
    "ax[1].set_ylabel(\"R²\")\n",
    "ax[1].set_xticks(np.arange(len(regions)))\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8dabba-7dc3-4ad3-a5d2-672cabf01da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions[:3]:\n",
    "    idx = lgbm_results[region][\"index_test\"]\n",
    "    yt  = lgbm_results[region][\"y_test\"]\n",
    "    yp  = lgbm_results[region][\"y_pred\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(idx, yt, label=\"Actual\")\n",
    "    plt.plot(idx, yp, label=\"LightGBM Predicted\")\n",
    "    plt.title(f\"{region} – Actual vs LightGBM Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663eccfd",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">6. Gradient Boosting ML model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e31469",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    gb_model = GradientBoostingRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb_model.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "\n",
    "    y_pred = gb_model.predict(d[\"X_test\"])\n",
    "    rmse   = np.sqrt(mean_squared_error(d[\"y_test\"], y_pred))\n",
    "    r2     = r2_score(d[\"y_test\"], y_pred)\n",
    "\n",
    "    gb_results[region] = {\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\":   r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": y_pred,\n",
    "        \"index_test\": d[\"index_test\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c471c-c20f-4577-89a7-d5df2076a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = list(gb_results)\n",
    "gb_rmses = [gb_results[r][\"rmse\"] for r in regions]\n",
    "gb_r2s   = [gb_results[r][\"r2\"]   for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "ax[0].bar(regions, gb_rmses)\n",
    "ax[0].set_title(\"Gradient Boosting – RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticks(np.arange(len(regions)))\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha=\"right\")\n",
    "\n",
    "# R²\n",
    "ax[1].bar(regions, gb_r2s)\n",
    "ax[1].set_title(\"Gradient Boosting – R² by Region\")\n",
    "ax[1].set_ylabel(\"R²\")\n",
    "ax[1].set_xticks(np.arange(len(regions)))\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa24a461-4411-465f-9cab-6556711fcb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions[:3]:\n",
    "    idx = gb_results[region][\"index_test\"]\n",
    "    yt  = gb_results[region][\"y_test\"]\n",
    "    yp  = gb_results[region][\"y_pred\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(idx, yt, label=\"Actual\")\n",
    "    plt.plot(idx, yp, label=\"GBR Predicted\")\n",
    "    plt.title(f\"{region} – Actual vs Gradient Boosting Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f9258",
   "metadata": {},
   "source": [
    "## <span style=\"color:pink\">7. Ridge regression ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095960e",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use the Ridge Regression (Ridge) machine learning algorithm to predict energy consumption for different regions based on weather and seasonal features. Ridge Regression is a regularized version of linear regression that adds an L2 penalty (the squared magnitude of coefficients) to the loss function, which helps to prevent overfitting and manage multicollinearity among features.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it trains a `Ridge` regression model using the training data (`X_train`, `y_train`).\n",
    "- It predicts energy values on the test set (`X_test`).\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use Ridge Regression?**\n",
    "- Ridge Regression is effective when there is multicollinearity (correlated features) in the data, as it shrinks the coefficients and reduces model variance.\n",
    "- The L2 regularization helps to prevent overfitting, especially when the number of features is large relative to the number of observations.\n",
    "- Ridge provides a balance between bias and variance, often improving generalization performance compared to standard linear regression.\n",
    "- It is simple, interpretable, and computationally efficient.\n",
    "\n",
    "In this notebook, Ridge Regression is used to model the relationship between temperature, seasonal effects, and energy consumption, providing a robust and regularized approach for regional energy demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    ridge_model = make_pipeline(\n",
    "        StandardScaler(),          # keeps coefficients well-scaled\n",
    "        Ridge(alpha=1.0, random_state=42)\n",
    "    )\n",
    "    ridge_model.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "\n",
    "    y_pred = ridge_model.predict(d[\"X_test\"])\n",
    "    rmse   = np.sqrt(mean_squared_error(d[\"y_test\"], y_pred))\n",
    "    r2     = r2_score(d[\"y_test\"], y_pred)\n",
    "\n",
    "    ridge_results[region] = {\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\":   r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": y_pred,\n",
    "        \"index_test\": d[\"index_test\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f407cb7-08f6-46c1-8147-8216f4f8b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions    = list(ridge_results)               # ensure sync\n",
    "ridge_rmses = [ridge_results[r][\"rmse\"] for r in regions]\n",
    "ridge_r2s   = [ridge_results[r][\"r2\"]   for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "ax[0].bar(regions, ridge_rmses)\n",
    "ax[0].set_title(\"Ridge Regression – RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticks(np.arange(len(regions)))\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha=\"right\")\n",
    "\n",
    "# R²\n",
    "ax[1].bar(regions, ridge_r2s)\n",
    "ax[1].set_title(\"Ridge Regression – R² by Region\")\n",
    "ax[1].set_ylabel(\"R²\")\n",
    "ax[1].set_xticks(np.arange(len(regions)))\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a810fb-ab2d-48da-8af8-b42506ed54e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions[:3]:\n",
    "    idx = ridge_results[region][\"index_test\"]\n",
    "    yt  = ridge_results[region][\"y_test\"]\n",
    "    yp  = ridge_results[region][\"y_pred\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(idx, yt, label=\"Actual\")\n",
    "    plt.plot(idx, yp, label=\"Ridge Predicted\")\n",
    "    plt.title(f\"{region} – Actual vs Ridge Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a776e",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">8. SVR pipeline ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd03ed5",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use a machine learning pipeline that combines feature scaling, feature selection, dimensionality reduction, and Support Vector Regression (SVR) to predict energy consumption for different regions. This approach leverages the strengths of each preprocessing step to improve the performance of SVR, as evidenced by the increased accuracy compared to a plain SVR model.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it constructs a `Pipeline` consisting of:\n",
    "    - `StandardScaler`: Standardizes features by removing the mean and scaling to unit variance, which is important for SVR since it is sensitive to the scale of input features.\n",
    "    - `SelectKBest`: Selects the best features based on univariate linear regression tests, helping to remove irrelevant or less informative features.\n",
    "    - `PCA`: Applies Principal Component Analysis to reduce dimensionality while retaining 95% of the variance, which can help to denoise the data and reduce overfitting.\n",
    "    - `SVR`: Fits a Support Vector Regression model with an RBF kernel to capture non-linear relationships.\n",
    "- The pipeline is trained on the training data and evaluated on the test set.\n",
    "- Model performance is assessed using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- Results are visualized for all regions, and actual vs. predicted energy values are plotted for selected regions.\n",
    "\n",
    "**Why does this pipeline improve SVR accuracy?**\n",
    "- **Scaling** ensures all features contribute equally to the SVR model, preventing features with larger scales from dominating.\n",
    "- **Feature selection** removes noise and irrelevant features, allowing the model to focus on the most predictive variables.\n",
    "- **Dimensionality reduction (PCA)** helps to further reduce noise and multicollinearity, making the SVR model more robust and less prone to overfitting.\n",
    "- **SVR** benefits from these preprocessing steps, leading to better generalization and improved accuracy, as seen in the higher R² scores.\n",
    "\n",
    "By integrating these preprocessing steps, the pipeline addresses common challenges in regression modeling and enhances the predictive power of SVR for regional energy demand forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a545df-773c-4b9e-b582-60a93d87f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_pipe_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    svr_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca',    PCA(n_components=0.95, svd_solver='full')),   # keep 95 % variance\n",
    "        ('svr',    SVR(kernel='rbf', C=10, gamma='scale', epsilon=0.1)),\n",
    "    ])\n",
    "    svr_pipe.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "\n",
    "    y_pred = svr_pipe.predict(d[\"X_test\"])\n",
    "    rmse   = np.sqrt(mean_squared_error(d[\"y_test\"], y_pred))\n",
    "    r2     = r2_score(d[\"y_test\"], y_pred)\n",
    "\n",
    "    svr_pipe_results[region] = {\n",
    "        \"rmse\":  rmse,\n",
    "        \"r2\":    r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": y_pred,\n",
    "        \"index_test\": d[\"index_test\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01aac84-68ae-42eb-b8a0-5044ecdcad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions        = list(svr_pipe_results)\n",
    "svr_pipe_rmses = [svr_pipe_results[r][\"rmse\"] for r in regions]\n",
    "svr_pipe_r2s   = [svr_pipe_results[r][\"r2\"]   for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "ax[0].bar(regions, svr_pipe_rmses)\n",
    "ax[0].set_title(\"SVR (PCA) – RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticks(np.arange(len(regions)))\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "# R²\n",
    "ax[1].bar(regions, svr_pipe_r2s)\n",
    "ax[1].set_title(\"SVR (PCA) – R² by Region\")\n",
    "ax[1].set_ylabel(\"R²\")\n",
    "ax[1].set_xticks(np.arange(len(regions)))\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15fad18-3fcb-47a7-a442-a793f8e2a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions[:3]:\n",
    "    idx = svr_pipe_results[region][\"index_test\"]\n",
    "    yt  = svr_pipe_results[region][\"y_test\"]\n",
    "    yp  = svr_pipe_results[region][\"y_pred\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(idx, yt, label=\"Actual\")\n",
    "    plt.plot(idx, yp, label=\"SVR (PCA) Predicted\")\n",
    "    plt.title(f\"{region} – Actual vs SVR (PCA) Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f1780",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">9. XGBoost + SVR pipeline ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26a1179",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use a stacking ensemble that combines an SVR pipeline and XGBoost to predict energy consumption for different regions based on weather and seasonal features. This approach leverages the strengths of both SVR (with preprocessing) and XGBoost by stacking their predictions, allowing a final XGBoost model to learn from both sets of outputs for improved accuracy.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it builds a `StackingRegressor` with two base estimators:\n",
    "    - An SVR pipeline that includes feature scaling, feature selection, and PCA for dimensionality reduction, followed by an SVR model.\n",
    "    - An XGBoost regressor.\n",
    "- The outputs of these base models are combined and used as input features for a final XGBoost regressor (the meta-learner).\n",
    "- The stacked model is trained on the training data and evaluated on the test set.\n",
    "- Model performance is assessed using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- Results are visualized for all regions, and actual vs. predicted energy values are plotted for selected regions.\n",
    "\n",
    "**Why does this stacking approach work?**\n",
    "- **Model diversity:** SVR and XGBoost capture different aspects of the data; SVR is effective for smooth, non-linear relationships after preprocessing, while XGBoost excels at handling complex interactions and outliers.\n",
    "- **Preprocessing pipeline:** The SVR pipeline ensures that features are well-scaled, relevant, and denoised before regression, which helps SVR perform optimally.\n",
    "- **Meta-learning:** The final XGBoost regressor (meta-learner) can learn how to best combine the predictions from both base models, correcting their individual weaknesses and leveraging their strengths.\n",
    "- **Reduced overfitting:** By combining multiple models, stacking can reduce the risk of overfitting compared to relying on a single model.\n",
    "- **Improved accuracy:** The ensemble often achieves better predictive performance than any individual model, as evidenced by higher R² scores and lower RMSE.\n",
    "\n",
    "This stacking ensemble provides a robust and flexible framework for regional energy demand forecasting, taking advantage of both advanced preprocessing and powerful machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de56aa82-fe5a-4fe0-a77f-cbd68b3155cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_xgb_stack_results = {}\n",
    "\n",
    "for region, d in data_dict.items():\n",
    "    # 1️⃣  SVR pipeline (scale → PCA → SVR)\n",
    "    svr_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca',    PCA(n_components=0.95, svd_solver='full')),\n",
    "        ('svr',    SVR(kernel='rbf', C=10, gamma='scale', epsilon=0.1)),\n",
    "    ])\n",
    "\n",
    "    # 2️⃣  Base XGB (no scaling needed)\n",
    "    xgb_base = XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.06,\n",
    "        max_depth=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    # 3️⃣  Final estimator (small XGB)\n",
    "    xgb_final = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    stack = StackingRegressor(\n",
    "        estimators=[('svr', svr_pipe), ('xgb', xgb_base)],\n",
    "        final_estimator=xgb_final,\n",
    "        passthrough=True\n",
    "    )\n",
    "\n",
    "    stack.fit(d[\"X_train\"], d[\"y_train\"])\n",
    "\n",
    "    y_pred = stack.predict(d[\"X_test\"])\n",
    "    rmse   = np.sqrt(mean_squared_error(d[\"y_test\"], y_pred))\n",
    "    r2     = r2_score(d[\"y_test\"], y_pred)\n",
    "\n",
    "    svr_xgb_stack_results[region] = {\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\":   r2,\n",
    "        \"y_test\": d[\"y_test\"],\n",
    "        \"y_pred\": y_pred,\n",
    "        \"index_test\": d[\"index_test\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3f9f3-e3e8-4ee4-adbf-1f215b2e3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions    = list(svr_xgb_stack_results)          # sync with actual results\n",
    "stack_rmses = [svr_xgb_stack_results[r][\"rmse\"] for r in regions]\n",
    "stack_r2s   = [svr_xgb_stack_results[r][\"r2\"]   for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RMSE\n",
    "ax[0].bar(regions, stack_rmses)\n",
    "ax[0].set_title(\"SVR + XGBoost Stack – RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticks(np.arange(len(regions)))\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "# R²\n",
    "ax[1].bar(regions, stack_r2s)\n",
    "ax[1].set_title(\"SVR + XGBoost Stack – R² by Region\")\n",
    "ax[1].set_ylabel(\"R²\")\n",
    "ax[1].set_xticks(np.arange(len(regions)))\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3699d04-ad89-4595-8e62-d0534f892ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions[:3]:\n",
    "    idx = svr_xgb_stack_results[region][\"index_test\"]\n",
    "    yt  = svr_xgb_stack_results[region][\"y_test\"]\n",
    "    yp  = svr_xgb_stack_results[region][\"y_pred\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(idx, yt, label=\"Actual\")\n",
    "    plt.plot(idx, yp, label=\"SVR+XGB Predicted\")\n",
    "    plt.title(f\"{region} – Actual vs SVR + XGB Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3b66c",
   "metadata": {},
   "source": [
    "## <span style=\"color:pink\">10. LSTM ML model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74625109",
   "metadata": {},
   "source": [
    "The code below demonstrates how to use a Long Short-Term Memory (LSTM) neural network to predict energy consumption for different regions based on weather, seasonal, and historical energy features. LSTM is a type of recurrent neural network (RNN) that is well-suited for modeling sequential and time series data due to its ability to capture long-term dependencies and temporal patterns.\n",
    "\n",
    "**What the code does:**\n",
    "- For each region, it transforms the feature and target arrays into sequences of a fixed length (`SEQ_LEN`), so the LSTM can learn from historical windows of data.\n",
    "- It builds and trains an LSTM-based neural network using the training sequences, with dropout for regularization and dense layers for additional learning capacity.\n",
    "- It evaluates model performance using RMSE (Root Mean Squared Error) and R² (coefficient of determination) metrics.\n",
    "- It visualizes the RMSE and R² scores for all regions.\n",
    "- It plots actual vs. predicted energy values for a few example regions.\n",
    "\n",
    "**Why use LSTM?**\n",
    "- LSTM networks are specifically designed to handle sequential data and can learn complex temporal relationships, making them ideal for time series forecasting.\n",
    "- They are effective at capturing both short-term and long-term dependencies in data, which is important for energy demand prediction where past consumption and weather patterns influence future values.\n",
    "- LSTMs can model non-linear and non-stationary processes, outperforming traditional models when the data exhibits complex temporal dynamics.\n",
    "- They are robust to noise and can generalize well when trained with sufficient data.\n",
    "\n",
    "In this notebook, LSTM is used to model the relationship between historical energy usage, temperature, seasonal effects, and future energy demand, providing a powerful deep learning approach for regional energy forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0224f394-e3c5-40fe-8d9a-6476952855d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)   # reproducibility\n",
    "\n",
    "SEQ_LEN = 7              # past 7 days → predict day 8\n",
    "BATCH  = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "def make_sequences(X, y, seq_len):\n",
    "    \"\"\"Cast tabular X, y into overlapping LSTM windows.\"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        xs.append(X[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "lstm_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a226d4-ed08-484f-8a6f-b9d2afb7b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region, d in data_dict.items():\n",
    "    # ── scale features ─────────────────────────────\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_scaled = scaler.fit_transform(d[\"X_train\"])\n",
    "    X_te_scaled = scaler.transform(d[\"X_test\"])\n",
    "\n",
    "    # ── build seq windows ──────────────────────────\n",
    "    X_tr_seq, y_tr_seq = make_sequences(X_tr_scaled, d[\"y_train\"], SEQ_LEN)\n",
    "    X_te_seq, y_te_seq = make_sequences(X_te_scaled, d[\"y_test\"],  SEQ_LEN)\n",
    "    idx_test_seq       = d[\"index_test\"][SEQ_LEN:]\n",
    "\n",
    "    # ── define LSTM model ──────────────────────────\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(64, activation='relu'), input_shape=(SEQ_LEN, X_tr_seq.shape[2])),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(1e-3), loss='mse')\n",
    "\n",
    "    # ── fit ────────────────────────────────────────\n",
    "    es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "    model.fit(\n",
    "        X_tr_seq, y_tr_seq,\n",
    "        validation_split=0.2,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH,\n",
    "        callbacks=[es],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # ── evaluate ───────────────────────────────────\n",
    "    y_pred = model.predict(X_te_seq, verbose=0).flatten()\n",
    "    rmse   = np.sqrt(mean_squared_error(y_te_seq, y_pred))\n",
    "    r2     = r2_score(y_te_seq, y_pred)\n",
    "\n",
    "    lstm_results[region] = {\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\":   r2,\n",
    "        \"y_test\": y_te_seq,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"index_test\": idx_test_seq,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a17080-5268-4779-994b-af4897c8b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions    = list(lstm_results)\n",
    "lstm_rmses = [lstm_results[r][\"rmse\"] for r in regions]\n",
    "lstm_r2s   = [lstm_results[r][\"r2\"]   for r in regions]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax[0].bar(regions, lstm_rmses)\n",
    "ax[0].set_title(\"LSTM – RMSE by Region\")\n",
    "ax[0].set_ylabel(\"RMSE\")\n",
    "ax[0].set_xticks(np.arange(len(regions)))\n",
    "ax[0].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "ax[1].bar(regions, lstm_r2s)\n",
    "ax[1].set_title(\"LSTM – R² by Region\")\n",
    "ax[1].set_ylabel(\"R²\")\n",
    "ax[1].set_xticks(np.arange(len(regions)))\n",
    "ax[1].set_xticklabels(regions, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9581a-ef52-40bb-af48-95bf24303731",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions[:3]:\n",
    "    idx = lstm_results[region][\"index_test\"]\n",
    "    yt  = lstm_results[region][\"y_test\"]\n",
    "    yp  = lstm_results[region][\"y_pred\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(idx, yt, label=\"Actual\")\n",
    "    plt.plot(idx, yp, label=\"LSTM Predicted\")\n",
    "    plt.title(f\"{region} – Actual vs LSTM Predicted Energy\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df7997",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare model names and their corresponding RMSE and R2 lists\n",
    "model_names = [\n",
    "    \"XGBoost\", \"SVR\", \"Random Forest\", \"Linear Regression\", \"LightGBM\",\n",
    "    \"Gradient Boosting\", \"Ridge Regression\", \"SVR (PCA)\", \"SVR+XGB Stack\", \"LSTM\"\n",
    "]\n",
    "\n",
    "# Gather RMSE and R2 lists for each model (order matches model_names)\n",
    "rmse_lists = [\n",
    "    rmses,                # XGBoost\n",
    "    svr_rmses,            # SVR\n",
    "    rf_rmses,             # Random Forest\n",
    "    lr_rmses,             # Linear Regression\n",
    "    lgbm_rmses,           # LightGBM\n",
    "    gb_rmses,             # Gradient Boosting\n",
    "    ridge_rmses,          # Ridge Regression\n",
    "    svr_pipe_rmses,       # SVR (PCA)\n",
    "    stack_rmses,          # SVR+XGB Stack\n",
    "    lstm_rmses            # LSTM\n",
    "]\n",
    "\n",
    "r2_lists = [\n",
    "    r2s,                  # XGBoost\n",
    "    svr_r2s,              # SVR\n",
    "    rf_r2s,               # Random Forest\n",
    "    lr_r2s,               # Linear Regression\n",
    "    lgbm_r2s,             # LightGBM\n",
    "    gb_r2s,               # Gradient Boosting\n",
    "    ridge_r2s,            # Ridge Regression\n",
    "    svr_pipe_r2s,         # SVR (PCA)\n",
    "    stack_r2s,            # SVR+XGB Stack\n",
    "    lstm_r2s              # LSTM\n",
    "]\n",
    "\n",
    "avg_rmse = [np.mean(lst) for lst in rmse_lists]\n",
    "std_rmse = [np.std(lst)  for lst in rmse_lists]\n",
    "avg_r2   = [np.mean(lst) for lst in r2_lists]\n",
    "std_r2   = [np.std(lst)  for lst in r2_lists]\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "compare_df = pd.DataFrame({\n",
    "    \"Model\"      : model_names,\n",
    "    \"RMSE (mean)\": avg_rmse,\n",
    "    \"RMSE (std)\" : std_rmse,\n",
    "    \"R² (mean)\"  : avg_r2,\n",
    "    \"R² (std)\"   : std_r2\n",
    "}).sort_values(\"RMSE (mean)\")          # lowest error first\n",
    "\n",
    "compare_df.reset_index(drop=True, inplace=True)\n",
    "compare_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b3ca2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c6164",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics: RMSE and R²\n",
    "\n",
    "When comparing machine learning models for energy prediction, two key metrics are used:\n",
    "\n",
    "### 1. Root Mean Squared Error (RMSE)\n",
    "- **Definition:** RMSE measures the average magnitude of the errors between predicted and actual values. It is the square root of the average of squared differences.\n",
    "- **Interpretation:**  \n",
    "    - **Low RMSE:** Indicates that the model's predictions are close to the actual values, meaning higher accuracy.\n",
    "    - **High RMSE:** Indicates larger errors in predictions, meaning lower accuracy.\n",
    "- **Units:** RMSE is in the same units as the target variable (here, energy).\n",
    "\n",
    "### 2. R² Score (Coefficient of Determination)\n",
    "- **Definition:** R² measures the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "- **Interpretation:**  \n",
    "    - **R² = 1:** Perfect prediction.\n",
    "    - **R² = 0:** Model predicts no better than the mean of the data.\n",
    "    - **R² < 0:** Model performs worse than simply predicting the mean.\n",
    "    - **Higher R²:** Indicates better model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparing Models\n",
    "\n",
    "- **Best Models:**  \n",
    "    - Models with **lowest RMSE** and **highest R²** are considered best, as they make predictions closest to the actual values and explain the most variance.\n",
    "    - In the results above, ensemble models like **XGBoost+SVR Stacking** and advanced pipelines (e.g., SVR Pipeline) generally achieve lower RMSE and higher R² across regions, indicating superior performance.\n",
    "    - Simpler models (e.g., plain SVR or Linear Regression) tend to have higher RMSE and lower R², showing less accurate predictions.\n",
    "\n",
    "- **Why Some Models Perform Better:**  \n",
    "    - **Ensemble and pipeline models** combine the strengths of multiple algorithms and preprocessing steps, capturing complex relationships and reducing overfitting.\n",
    "    - **Tree-based models** (like XGBoost, Random Forest, LightGBM) handle non-linearities and interactions well, which are common in energy data.\n",
    "    - **Regularized models** (like Ridge Regression) help when features are correlated, but may not capture complex patterns as well as ensembles.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "- **Low RMSE and high R²** indicate a model is making accurate and reliable predictions.\n",
    "- **Model selection** should be based on these metrics, with preference for models that consistently perform well across all regions.\n",
    "- The results show that **stacking ensembles and advanced pipelines** provide the best predictive performance for regional energy forecasting in this workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
